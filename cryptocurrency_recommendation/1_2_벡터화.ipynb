{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-2. 벡터화",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNn4dhaf2hFi"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdE_-DEP3SPq",
        "outputId": "34766180-916e-4bcd-b09f-9dd70c737f87"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "import collections\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ucR3cAN3Wq6",
        "outputId": "62191328-2230-44a3-a46a-18d57ea82c41"
      },
      "source": [
        "eng = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/3rd project/data/eng/tweets_labelled_09042020_16072020.csv', sep=';').set_index('id')\n",
        "eng.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyq6pZ-Jtfvg"
      },
      "source": [
        "eng = eng[eng['sentiment'].notnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7Awluy-DJ0"
      },
      "source": [
        "ticker_pattern = re.compile(r'(^\\$[A-Z]+|^\\$ES_F)')\n",
        "ht_pattern = re.compile(r'#\\w+')\n",
        "\n",
        "ticker_dic = collections.defaultdict(int)\n",
        "ht_dic = collections.defaultdict(int)\n",
        "\n",
        "for text in eng['text']:\n",
        "    for word in text.split():\n",
        "        if ticker_pattern.fullmatch(word) is not None:\n",
        "            ticker_dic[word[1:]] += 1\n",
        "\n",
        "            word = word.lower()\n",
        "            if ht_pattern.fullmatch(word) is not None:\n",
        "                ht_dic[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpH4vKfyH8MP"
      },
      "source": [
        "charonly = re.compile(r'[^a-zA-Z\\s]')\n",
        "handle_pattern = re.compile(r'@\\w+')\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                        u\"\\U00002702-\\U000027B0\"\n",
        "                        u\"\\U000024C2-\\U0001F251\"\n",
        "                        \"]+\", flags=re.UNICODE)\n",
        "url_pattern = re.compile(\n",
        "    'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "pic_pattern = re.compile('pic\\.twitter\\.com/.{10}')\n",
        "special_code = re.compile(r'(&amp;|&gt;|&lt;)')\n",
        "tag_pattern = re.compile(r'<.*?>')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english')).union(\n",
        "    {'rt', 'retweet', 'RT', 'Retweet', 'RETWEET'})\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def hashtag(phrase):\n",
        "    return ht_pattern.sub(' ', phrase)\n",
        "\n",
        "def remove_ticker(phrase):\n",
        "    return ticker_pattern.sub('', phrase)\n",
        "    \n",
        "def specialcode(phrase):\n",
        "    return special_code.sub(' ', phrase)\n",
        "\n",
        "def emoji(phrase):\n",
        "    return emoji_pattern.sub(' ', phrase)\n",
        "\n",
        "def url(phrase):\n",
        "    return url_pattern.sub('', phrase)\n",
        "\n",
        "def pic(phrase):\n",
        "    return pic_pattern.sub('', phrase)\n",
        "\n",
        "def html_tag(phrase):\n",
        "    return tag_pattern.sub(' ', phrase)\n",
        "\n",
        "def handle(phrase):\n",
        "    return handle_pattern.sub('', phrase)\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    \n",
        "    # DIS, ticker symbol of Disney, is interpreted as the plural of \"DI\" \n",
        "    # in WordCloud, so I converted it to Disney\n",
        "    phrase = re.sub('DIS', 'Disney', phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"(he|He)\\'s\", \"he is\", phrase)\n",
        "    phrase = re.sub(r\"(she|She)\\'s\", \"she is\", phrase)\n",
        "    phrase = re.sub(r\"(it|It)\\'s\", \"it is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"(\\'ve|has)\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def onlychar(phrase):\n",
        "    return charonly.sub('', phrase)\n",
        "\n",
        "def remove_stopwords(phrase):\n",
        "    return \" \".join([word for word in str(phrase).split()\\\n",
        "                     if word not in STOPWORDS])\n",
        "\n",
        "def tokenize_stem(phrase):   \n",
        "    tokens = word_tokenize(phrase)\n",
        "    stem_words =[]\n",
        "    for token in tokens:\n",
        "        word = lemmatizer.lemmatize(token)\n",
        "        stem_words.append(word)        \n",
        "    buf = ' '.join(stem_words)    \n",
        "    return buf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wARDi25oYR0E"
      },
      "source": [
        "def arrange_text(ds):\n",
        "    ds['text2'] = ds['text'].apply(emoji)\n",
        "    ds['text2'] = ds['text2'].apply(handle)\n",
        "    ds['text2'] = ds['text2'].apply(specialcode)\n",
        "    ds['text2'] = ds['text2'].apply(hashtag)\n",
        "    ds['text2'] = ds['text2'].apply(url)\n",
        "    ds['text2'] = ds['text2'].apply(pic)\n",
        "    ds['text2'] = ds['text2'].apply(html_tag)\n",
        "    ds['text2'] = ds['text2'].apply(onlychar)\n",
        "    ds['text2'] = ds['text2'].apply(decontracted)\n",
        "    ds['text2'] = ds['text2'].apply(onlychar)\n",
        "    ds['text2'] = ds['text2'].apply(tokenize_stem)\n",
        "    ds['text2'] = ds['text2'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABiZYts2YVqf"
      },
      "source": [
        "arrange_text(eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Qx8aGvhJAueE",
        "outputId": "f6c03ca4-3f24-4f37-86dd-ccd541a3b485"
      },
      "source": [
        "eng = eng.replace({'sentiment': 'positive'}, {'sentiment': 0})\n",
        "eng = eng.replace({'sentiment': 'neutral'}, {'sentiment': 1})\n",
        "eng = eng.replace({'sentiment': 'negative'}, {'sentiment': 2})\n",
        "\n",
        "eng.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>77522</th>\n",
              "      <td>2020-04-15 01:03:46+00:00</td>\n",
              "      <td>RT @RobertBeadles: Yo💥\\nEnter to WIN 1,000 Mon...</td>\n",
              "      <td>0</td>\n",
              "      <td>Yo Enter WIN Monarch Tokens US Stock Market Cr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661634</th>\n",
              "      <td>2020-06-25 06:20:06+00:00</td>\n",
              "      <td>#SriLanka surcharge on fuel removed!\\n⛽📉\\nThe ...</td>\n",
              "      <td>2</td>\n",
              "      <td>surcharge fuel removed The surcharge Rs impose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413231</th>\n",
              "      <td>2020-06-04 15:41:45+00:00</td>\n",
              "      <td>Net issuance increases to fund fiscal programs...</td>\n",
              "      <td>0</td>\n",
              "      <td>Net issuance increase fund fiscal program yiel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760262</th>\n",
              "      <td>2020-07-03 19:39:35+00:00</td>\n",
              "      <td>RT @bentboolean: How much of Amazon's traffic ...</td>\n",
              "      <td>0</td>\n",
              "      <td>How much Amazons traffic served Fastly Help u ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830153</th>\n",
              "      <td>2020-07-09 14:39:14+00:00</td>\n",
              "      <td>$AMD Ryzen 4000 desktop CPUs looking ‘great’ a...</td>\n",
              "      <td>0</td>\n",
              "      <td>AMD Ryzen desktop CPUs looking great track launch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       created_at  ...                                              text2\n",
              "id                                 ...                                                   \n",
              "77522   2020-04-15 01:03:46+00:00  ...  Yo Enter WIN Monarch Tokens US Stock Market Cr...\n",
              "661634  2020-06-25 06:20:06+00:00  ...  surcharge fuel removed The surcharge Rs impose...\n",
              "413231  2020-06-04 15:41:45+00:00  ...  Net issuance increase fund fiscal program yiel...\n",
              "760262  2020-07-03 19:39:35+00:00  ...  How much Amazons traffic served Fastly Help u ...\n",
              "830153  2020-07-09 14:39:14+00:00  ...  AMD Ryzen desktop CPUs looking great track launch\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk293S1Wwe7r"
      },
      "source": [
        "train = eng[:1000]\n",
        "test = eng[1000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4G84rnJ2NrF"
      },
      "source": [
        "train_X = train['text2']\n",
        "train_y = train['sentiment']\n",
        "test_X = test['text2']\n",
        "test_y = test['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz8dpA2j45iH"
      },
      "source": [
        "## Counter 벡터화\n",
        "* https://dschloe.github.io/python/nlp/ch04_sentiment_analysis/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lnK5EJ62wvO",
        "outputId": "6c5eb95b-8356-4be1-bb7f-3760d1986390"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                     ('cnt_vect', CountVectorizer(stop_words = 'english', ngram_range=(1, 2))),\n",
        "                     ('lr_clf', LogisticRegression(C=10))])\n",
        "\n",
        "pipeline.fit(train_X, train_y)\n",
        "pred = pipeline.predict(test_X)\n",
        "pred_probs = pipeline.predict_proba(test_X)\n",
        "\n",
        "print('예측 정확도는 {0:.4f}'.format(accuracy_score(test_y, pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도는 0.5467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv3Z-cKK48Cx"
      },
      "source": [
        "## TF-IDF 벡터화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0NEiEsN4bTC",
        "outputId": "04951f43-8115-4deb-d070-8255c1f550e7"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "                     ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
        "                     ('lr_clf', LogisticRegression(C=10))])\n",
        "\n",
        "pipeline.fit(train_X, train_y)\n",
        "pred = pipeline.predict(test_X)\n",
        "\n",
        "print('예측 정확도는 {0:.4f}'.format(accuracy_score(test_y, pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도는 0.5733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgyFczSoVHnM",
        "outputId": "5227347b-308b-47f2-898b-774ee0563796"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                     ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
        "                     ('dt_clf', DecisionTreeClassifier())])\n",
        "\n",
        "pipeline.fit(train_X, train_y)\n",
        "pred = pipeline.predict(test_X)\n",
        "\n",
        "print('예측 정확도는 {0:.4f}'.format(accuracy_score(test_y, pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도는 0.4567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsueF7LsVYQd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}